{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pE7yHJj2w7ktgNStPkqqL_avQSHOsMvE",
      "authorship_tag": "ABX9TyPkGZ8OqQXZbQNzn3Lrcu6a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeeshanAhmed95/MNIST-Classification-Keras/blob/main/MNIST_Classification_using_Sklearn_and_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Approach :**\n",
        "1. **Load all the libraries and Data**\n",
        "2. **Data Preparations**\n",
        "3. **Data Visualization**\n",
        "4. **Analysis** -\n",
        "      - ***Sklearn***\n",
        "           - Perceptron\n",
        "           - Multi Layer Perceptron\n",
        "\n",
        "      - ***Keras***  \n",
        "          - Set the model (sequential/functional): we will use sequential initially\n",
        "\n",
        "          - Set up the layers: Dense and Dropout\n",
        "\n",
        "          - Compile the model: Optimizer, Loss function, Batch\n",
        "\n",
        "          - Fit the model: (x,y, epochs)\n",
        "\n",
        "          - Predict with the model (validation dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QGwxSxGLnNQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chatgpt doubts Q&A :**\n",
        "\n",
        "https://chat.openai.com/share/00233780-2750-48d4-8ebf-1ea843c2b97b"
      ],
      "metadata": {
        "id": "eYy9A97_MULp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Articles to refer:**\n",
        "\n",
        "######**Bias and Variance**: https://www.tutorialspoint.com/difference-between-bias-and-variance-in-machine-learning"
      ],
      "metadata": {
        "id": "XzPnOH3Egd7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**DAY - 1**"
      ],
      "metadata": {
        "id": "ncSHXafibJhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjG0b1IR9gF1",
        "outputId": "4ac672cf-1234-4a99-b197-291b9e88d1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/digit-recognizer.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/digit-recognizer.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "##########kera libraries and Modules#######\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "GKsEUypW_94f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "DS0jTcRwoJ4m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.skew()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPbVUA7bt7Gb",
        "outputId": "97693098-d243-48d0-9822-930b14084133"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label         0.026479\n",
              "pixel0        0.000000\n",
              "pixel1        0.000000\n",
              "pixel2        0.000000\n",
              "pixel3        0.000000\n",
              "               ...    \n",
              "pixel779    145.149671\n",
              "pixel780      0.000000\n",
              "pixel781      0.000000\n",
              "pixel782      0.000000\n",
              "pixel783      0.000000\n",
              "Length: 785, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DAY - 2**"
      ],
      "metadata": {
        "id": "EnOQU2VSbStz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jamboard.google.com/d/1RLwFWpXLR3PYTXv6UTw9QUYvJcK3rK61SUGYcHjFwgg/viewer\n",
        "\n",
        "https://www.geeksforgeeks.org/difference-between-loc-and-iloc-in-pandas-dataframe/"
      ],
      "metadata": {
        "id": "Usr3AOOkbZ0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"pixel{}\".format(pixel_num) for pixel_num in range(0,784)]\n",
        "rows_to_examine = 3\n",
        "image_data = np.reshape(train[features][rows_to_examine:rows_to_examine+1].to_numpy(),(28,28))\n",
        "plt.imshow(image_data, cmap = \"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "TKi8biUTbcUK",
        "outputId": "1282967c-e6c6-43af-a44c-f25595af0c22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b3b794b54e0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "JXIcM7vRdZSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9f37f6-70bc-478a-954e-242882c4c049"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pixel0',\n",
              " 'pixel1',\n",
              " 'pixel2',\n",
              " 'pixel3',\n",
              " 'pixel4',\n",
              " 'pixel5',\n",
              " 'pixel6',\n",
              " 'pixel7',\n",
              " 'pixel8',\n",
              " 'pixel9',\n",
              " 'pixel10',\n",
              " 'pixel11',\n",
              " 'pixel12',\n",
              " 'pixel13',\n",
              " 'pixel14',\n",
              " 'pixel15',\n",
              " 'pixel16',\n",
              " 'pixel17',\n",
              " 'pixel18',\n",
              " 'pixel19',\n",
              " 'pixel20',\n",
              " 'pixel21',\n",
              " 'pixel22',\n",
              " 'pixel23',\n",
              " 'pixel24',\n",
              " 'pixel25',\n",
              " 'pixel26',\n",
              " 'pixel27',\n",
              " 'pixel28',\n",
              " 'pixel29',\n",
              " 'pixel30',\n",
              " 'pixel31',\n",
              " 'pixel32',\n",
              " 'pixel33',\n",
              " 'pixel34',\n",
              " 'pixel35',\n",
              " 'pixel36',\n",
              " 'pixel37',\n",
              " 'pixel38',\n",
              " 'pixel39',\n",
              " 'pixel40',\n",
              " 'pixel41',\n",
              " 'pixel42',\n",
              " 'pixel43',\n",
              " 'pixel44',\n",
              " 'pixel45',\n",
              " 'pixel46',\n",
              " 'pixel47',\n",
              " 'pixel48',\n",
              " 'pixel49',\n",
              " 'pixel50',\n",
              " 'pixel51',\n",
              " 'pixel52',\n",
              " 'pixel53',\n",
              " 'pixel54',\n",
              " 'pixel55',\n",
              " 'pixel56',\n",
              " 'pixel57',\n",
              " 'pixel58',\n",
              " 'pixel59',\n",
              " 'pixel60',\n",
              " 'pixel61',\n",
              " 'pixel62',\n",
              " 'pixel63',\n",
              " 'pixel64',\n",
              " 'pixel65',\n",
              " 'pixel66',\n",
              " 'pixel67',\n",
              " 'pixel68',\n",
              " 'pixel69',\n",
              " 'pixel70',\n",
              " 'pixel71',\n",
              " 'pixel72',\n",
              " 'pixel73',\n",
              " 'pixel74',\n",
              " 'pixel75',\n",
              " 'pixel76',\n",
              " 'pixel77',\n",
              " 'pixel78',\n",
              " 'pixel79',\n",
              " 'pixel80',\n",
              " 'pixel81',\n",
              " 'pixel82',\n",
              " 'pixel83',\n",
              " 'pixel84',\n",
              " 'pixel85',\n",
              " 'pixel86',\n",
              " 'pixel87',\n",
              " 'pixel88',\n",
              " 'pixel89',\n",
              " 'pixel90',\n",
              " 'pixel91',\n",
              " 'pixel92',\n",
              " 'pixel93',\n",
              " 'pixel94',\n",
              " 'pixel95',\n",
              " 'pixel96',\n",
              " 'pixel97',\n",
              " 'pixel98',\n",
              " 'pixel99',\n",
              " 'pixel100',\n",
              " 'pixel101',\n",
              " 'pixel102',\n",
              " 'pixel103',\n",
              " 'pixel104',\n",
              " 'pixel105',\n",
              " 'pixel106',\n",
              " 'pixel107',\n",
              " 'pixel108',\n",
              " 'pixel109',\n",
              " 'pixel110',\n",
              " 'pixel111',\n",
              " 'pixel112',\n",
              " 'pixel113',\n",
              " 'pixel114',\n",
              " 'pixel115',\n",
              " 'pixel116',\n",
              " 'pixel117',\n",
              " 'pixel118',\n",
              " 'pixel119',\n",
              " 'pixel120',\n",
              " 'pixel121',\n",
              " 'pixel122',\n",
              " 'pixel123',\n",
              " 'pixel124',\n",
              " 'pixel125',\n",
              " 'pixel126',\n",
              " 'pixel127',\n",
              " 'pixel128',\n",
              " 'pixel129',\n",
              " 'pixel130',\n",
              " 'pixel131',\n",
              " 'pixel132',\n",
              " 'pixel133',\n",
              " 'pixel134',\n",
              " 'pixel135',\n",
              " 'pixel136',\n",
              " 'pixel137',\n",
              " 'pixel138',\n",
              " 'pixel139',\n",
              " 'pixel140',\n",
              " 'pixel141',\n",
              " 'pixel142',\n",
              " 'pixel143',\n",
              " 'pixel144',\n",
              " 'pixel145',\n",
              " 'pixel146',\n",
              " 'pixel147',\n",
              " 'pixel148',\n",
              " 'pixel149',\n",
              " 'pixel150',\n",
              " 'pixel151',\n",
              " 'pixel152',\n",
              " 'pixel153',\n",
              " 'pixel154',\n",
              " 'pixel155',\n",
              " 'pixel156',\n",
              " 'pixel157',\n",
              " 'pixel158',\n",
              " 'pixel159',\n",
              " 'pixel160',\n",
              " 'pixel161',\n",
              " 'pixel162',\n",
              " 'pixel163',\n",
              " 'pixel164',\n",
              " 'pixel165',\n",
              " 'pixel166',\n",
              " 'pixel167',\n",
              " 'pixel168',\n",
              " 'pixel169',\n",
              " 'pixel170',\n",
              " 'pixel171',\n",
              " 'pixel172',\n",
              " 'pixel173',\n",
              " 'pixel174',\n",
              " 'pixel175',\n",
              " 'pixel176',\n",
              " 'pixel177',\n",
              " 'pixel178',\n",
              " 'pixel179',\n",
              " 'pixel180',\n",
              " 'pixel181',\n",
              " 'pixel182',\n",
              " 'pixel183',\n",
              " 'pixel184',\n",
              " 'pixel185',\n",
              " 'pixel186',\n",
              " 'pixel187',\n",
              " 'pixel188',\n",
              " 'pixel189',\n",
              " 'pixel190',\n",
              " 'pixel191',\n",
              " 'pixel192',\n",
              " 'pixel193',\n",
              " 'pixel194',\n",
              " 'pixel195',\n",
              " 'pixel196',\n",
              " 'pixel197',\n",
              " 'pixel198',\n",
              " 'pixel199',\n",
              " 'pixel200',\n",
              " 'pixel201',\n",
              " 'pixel202',\n",
              " 'pixel203',\n",
              " 'pixel204',\n",
              " 'pixel205',\n",
              " 'pixel206',\n",
              " 'pixel207',\n",
              " 'pixel208',\n",
              " 'pixel209',\n",
              " 'pixel210',\n",
              " 'pixel211',\n",
              " 'pixel212',\n",
              " 'pixel213',\n",
              " 'pixel214',\n",
              " 'pixel215',\n",
              " 'pixel216',\n",
              " 'pixel217',\n",
              " 'pixel218',\n",
              " 'pixel219',\n",
              " 'pixel220',\n",
              " 'pixel221',\n",
              " 'pixel222',\n",
              " 'pixel223',\n",
              " 'pixel224',\n",
              " 'pixel225',\n",
              " 'pixel226',\n",
              " 'pixel227',\n",
              " 'pixel228',\n",
              " 'pixel229',\n",
              " 'pixel230',\n",
              " 'pixel231',\n",
              " 'pixel232',\n",
              " 'pixel233',\n",
              " 'pixel234',\n",
              " 'pixel235',\n",
              " 'pixel236',\n",
              " 'pixel237',\n",
              " 'pixel238',\n",
              " 'pixel239',\n",
              " 'pixel240',\n",
              " 'pixel241',\n",
              " 'pixel242',\n",
              " 'pixel243',\n",
              " 'pixel244',\n",
              " 'pixel245',\n",
              " 'pixel246',\n",
              " 'pixel247',\n",
              " 'pixel248',\n",
              " 'pixel249',\n",
              " 'pixel250',\n",
              " 'pixel251',\n",
              " 'pixel252',\n",
              " 'pixel253',\n",
              " 'pixel254',\n",
              " 'pixel255',\n",
              " 'pixel256',\n",
              " 'pixel257',\n",
              " 'pixel258',\n",
              " 'pixel259',\n",
              " 'pixel260',\n",
              " 'pixel261',\n",
              " 'pixel262',\n",
              " 'pixel263',\n",
              " 'pixel264',\n",
              " 'pixel265',\n",
              " 'pixel266',\n",
              " 'pixel267',\n",
              " 'pixel268',\n",
              " 'pixel269',\n",
              " 'pixel270',\n",
              " 'pixel271',\n",
              " 'pixel272',\n",
              " 'pixel273',\n",
              " 'pixel274',\n",
              " 'pixel275',\n",
              " 'pixel276',\n",
              " 'pixel277',\n",
              " 'pixel278',\n",
              " 'pixel279',\n",
              " 'pixel280',\n",
              " 'pixel281',\n",
              " 'pixel282',\n",
              " 'pixel283',\n",
              " 'pixel284',\n",
              " 'pixel285',\n",
              " 'pixel286',\n",
              " 'pixel287',\n",
              " 'pixel288',\n",
              " 'pixel289',\n",
              " 'pixel290',\n",
              " 'pixel291',\n",
              " 'pixel292',\n",
              " 'pixel293',\n",
              " 'pixel294',\n",
              " 'pixel295',\n",
              " 'pixel296',\n",
              " 'pixel297',\n",
              " 'pixel298',\n",
              " 'pixel299',\n",
              " 'pixel300',\n",
              " 'pixel301',\n",
              " 'pixel302',\n",
              " 'pixel303',\n",
              " 'pixel304',\n",
              " 'pixel305',\n",
              " 'pixel306',\n",
              " 'pixel307',\n",
              " 'pixel308',\n",
              " 'pixel309',\n",
              " 'pixel310',\n",
              " 'pixel311',\n",
              " 'pixel312',\n",
              " 'pixel313',\n",
              " 'pixel314',\n",
              " 'pixel315',\n",
              " 'pixel316',\n",
              " 'pixel317',\n",
              " 'pixel318',\n",
              " 'pixel319',\n",
              " 'pixel320',\n",
              " 'pixel321',\n",
              " 'pixel322',\n",
              " 'pixel323',\n",
              " 'pixel324',\n",
              " 'pixel325',\n",
              " 'pixel326',\n",
              " 'pixel327',\n",
              " 'pixel328',\n",
              " 'pixel329',\n",
              " 'pixel330',\n",
              " 'pixel331',\n",
              " 'pixel332',\n",
              " 'pixel333',\n",
              " 'pixel334',\n",
              " 'pixel335',\n",
              " 'pixel336',\n",
              " 'pixel337',\n",
              " 'pixel338',\n",
              " 'pixel339',\n",
              " 'pixel340',\n",
              " 'pixel341',\n",
              " 'pixel342',\n",
              " 'pixel343',\n",
              " 'pixel344',\n",
              " 'pixel345',\n",
              " 'pixel346',\n",
              " 'pixel347',\n",
              " 'pixel348',\n",
              " 'pixel349',\n",
              " 'pixel350',\n",
              " 'pixel351',\n",
              " 'pixel352',\n",
              " 'pixel353',\n",
              " 'pixel354',\n",
              " 'pixel355',\n",
              " 'pixel356',\n",
              " 'pixel357',\n",
              " 'pixel358',\n",
              " 'pixel359',\n",
              " 'pixel360',\n",
              " 'pixel361',\n",
              " 'pixel362',\n",
              " 'pixel363',\n",
              " 'pixel364',\n",
              " 'pixel365',\n",
              " 'pixel366',\n",
              " 'pixel367',\n",
              " 'pixel368',\n",
              " 'pixel369',\n",
              " 'pixel370',\n",
              " 'pixel371',\n",
              " 'pixel372',\n",
              " 'pixel373',\n",
              " 'pixel374',\n",
              " 'pixel375',\n",
              " 'pixel376',\n",
              " 'pixel377',\n",
              " 'pixel378',\n",
              " 'pixel379',\n",
              " 'pixel380',\n",
              " 'pixel381',\n",
              " 'pixel382',\n",
              " 'pixel383',\n",
              " 'pixel384',\n",
              " 'pixel385',\n",
              " 'pixel386',\n",
              " 'pixel387',\n",
              " 'pixel388',\n",
              " 'pixel389',\n",
              " 'pixel390',\n",
              " 'pixel391',\n",
              " 'pixel392',\n",
              " 'pixel393',\n",
              " 'pixel394',\n",
              " 'pixel395',\n",
              " 'pixel396',\n",
              " 'pixel397',\n",
              " 'pixel398',\n",
              " 'pixel399',\n",
              " 'pixel400',\n",
              " 'pixel401',\n",
              " 'pixel402',\n",
              " 'pixel403',\n",
              " 'pixel404',\n",
              " 'pixel405',\n",
              " 'pixel406',\n",
              " 'pixel407',\n",
              " 'pixel408',\n",
              " 'pixel409',\n",
              " 'pixel410',\n",
              " 'pixel411',\n",
              " 'pixel412',\n",
              " 'pixel413',\n",
              " 'pixel414',\n",
              " 'pixel415',\n",
              " 'pixel416',\n",
              " 'pixel417',\n",
              " 'pixel418',\n",
              " 'pixel419',\n",
              " 'pixel420',\n",
              " 'pixel421',\n",
              " 'pixel422',\n",
              " 'pixel423',\n",
              " 'pixel424',\n",
              " 'pixel425',\n",
              " 'pixel426',\n",
              " 'pixel427',\n",
              " 'pixel428',\n",
              " 'pixel429',\n",
              " 'pixel430',\n",
              " 'pixel431',\n",
              " 'pixel432',\n",
              " 'pixel433',\n",
              " 'pixel434',\n",
              " 'pixel435',\n",
              " 'pixel436',\n",
              " 'pixel437',\n",
              " 'pixel438',\n",
              " 'pixel439',\n",
              " 'pixel440',\n",
              " 'pixel441',\n",
              " 'pixel442',\n",
              " 'pixel443',\n",
              " 'pixel444',\n",
              " 'pixel445',\n",
              " 'pixel446',\n",
              " 'pixel447',\n",
              " 'pixel448',\n",
              " 'pixel449',\n",
              " 'pixel450',\n",
              " 'pixel451',\n",
              " 'pixel452',\n",
              " 'pixel453',\n",
              " 'pixel454',\n",
              " 'pixel455',\n",
              " 'pixel456',\n",
              " 'pixel457',\n",
              " 'pixel458',\n",
              " 'pixel459',\n",
              " 'pixel460',\n",
              " 'pixel461',\n",
              " 'pixel462',\n",
              " 'pixel463',\n",
              " 'pixel464',\n",
              " 'pixel465',\n",
              " 'pixel466',\n",
              " 'pixel467',\n",
              " 'pixel468',\n",
              " 'pixel469',\n",
              " 'pixel470',\n",
              " 'pixel471',\n",
              " 'pixel472',\n",
              " 'pixel473',\n",
              " 'pixel474',\n",
              " 'pixel475',\n",
              " 'pixel476',\n",
              " 'pixel477',\n",
              " 'pixel478',\n",
              " 'pixel479',\n",
              " 'pixel480',\n",
              " 'pixel481',\n",
              " 'pixel482',\n",
              " 'pixel483',\n",
              " 'pixel484',\n",
              " 'pixel485',\n",
              " 'pixel486',\n",
              " 'pixel487',\n",
              " 'pixel488',\n",
              " 'pixel489',\n",
              " 'pixel490',\n",
              " 'pixel491',\n",
              " 'pixel492',\n",
              " 'pixel493',\n",
              " 'pixel494',\n",
              " 'pixel495',\n",
              " 'pixel496',\n",
              " 'pixel497',\n",
              " 'pixel498',\n",
              " 'pixel499',\n",
              " 'pixel500',\n",
              " 'pixel501',\n",
              " 'pixel502',\n",
              " 'pixel503',\n",
              " 'pixel504',\n",
              " 'pixel505',\n",
              " 'pixel506',\n",
              " 'pixel507',\n",
              " 'pixel508',\n",
              " 'pixel509',\n",
              " 'pixel510',\n",
              " 'pixel511',\n",
              " 'pixel512',\n",
              " 'pixel513',\n",
              " 'pixel514',\n",
              " 'pixel515',\n",
              " 'pixel516',\n",
              " 'pixel517',\n",
              " 'pixel518',\n",
              " 'pixel519',\n",
              " 'pixel520',\n",
              " 'pixel521',\n",
              " 'pixel522',\n",
              " 'pixel523',\n",
              " 'pixel524',\n",
              " 'pixel525',\n",
              " 'pixel526',\n",
              " 'pixel527',\n",
              " 'pixel528',\n",
              " 'pixel529',\n",
              " 'pixel530',\n",
              " 'pixel531',\n",
              " 'pixel532',\n",
              " 'pixel533',\n",
              " 'pixel534',\n",
              " 'pixel535',\n",
              " 'pixel536',\n",
              " 'pixel537',\n",
              " 'pixel538',\n",
              " 'pixel539',\n",
              " 'pixel540',\n",
              " 'pixel541',\n",
              " 'pixel542',\n",
              " 'pixel543',\n",
              " 'pixel544',\n",
              " 'pixel545',\n",
              " 'pixel546',\n",
              " 'pixel547',\n",
              " 'pixel548',\n",
              " 'pixel549',\n",
              " 'pixel550',\n",
              " 'pixel551',\n",
              " 'pixel552',\n",
              " 'pixel553',\n",
              " 'pixel554',\n",
              " 'pixel555',\n",
              " 'pixel556',\n",
              " 'pixel557',\n",
              " 'pixel558',\n",
              " 'pixel559',\n",
              " 'pixel560',\n",
              " 'pixel561',\n",
              " 'pixel562',\n",
              " 'pixel563',\n",
              " 'pixel564',\n",
              " 'pixel565',\n",
              " 'pixel566',\n",
              " 'pixel567',\n",
              " 'pixel568',\n",
              " 'pixel569',\n",
              " 'pixel570',\n",
              " 'pixel571',\n",
              " 'pixel572',\n",
              " 'pixel573',\n",
              " 'pixel574',\n",
              " 'pixel575',\n",
              " 'pixel576',\n",
              " 'pixel577',\n",
              " 'pixel578',\n",
              " 'pixel579',\n",
              " 'pixel580',\n",
              " 'pixel581',\n",
              " 'pixel582',\n",
              " 'pixel583',\n",
              " 'pixel584',\n",
              " 'pixel585',\n",
              " 'pixel586',\n",
              " 'pixel587',\n",
              " 'pixel588',\n",
              " 'pixel589',\n",
              " 'pixel590',\n",
              " 'pixel591',\n",
              " 'pixel592',\n",
              " 'pixel593',\n",
              " 'pixel594',\n",
              " 'pixel595',\n",
              " 'pixel596',\n",
              " 'pixel597',\n",
              " 'pixel598',\n",
              " 'pixel599',\n",
              " 'pixel600',\n",
              " 'pixel601',\n",
              " 'pixel602',\n",
              " 'pixel603',\n",
              " 'pixel604',\n",
              " 'pixel605',\n",
              " 'pixel606',\n",
              " 'pixel607',\n",
              " 'pixel608',\n",
              " 'pixel609',\n",
              " 'pixel610',\n",
              " 'pixel611',\n",
              " 'pixel612',\n",
              " 'pixel613',\n",
              " 'pixel614',\n",
              " 'pixel615',\n",
              " 'pixel616',\n",
              " 'pixel617',\n",
              " 'pixel618',\n",
              " 'pixel619',\n",
              " 'pixel620',\n",
              " 'pixel621',\n",
              " 'pixel622',\n",
              " 'pixel623',\n",
              " 'pixel624',\n",
              " 'pixel625',\n",
              " 'pixel626',\n",
              " 'pixel627',\n",
              " 'pixel628',\n",
              " 'pixel629',\n",
              " 'pixel630',\n",
              " 'pixel631',\n",
              " 'pixel632',\n",
              " 'pixel633',\n",
              " 'pixel634',\n",
              " 'pixel635',\n",
              " 'pixel636',\n",
              " 'pixel637',\n",
              " 'pixel638',\n",
              " 'pixel639',\n",
              " 'pixel640',\n",
              " 'pixel641',\n",
              " 'pixel642',\n",
              " 'pixel643',\n",
              " 'pixel644',\n",
              " 'pixel645',\n",
              " 'pixel646',\n",
              " 'pixel647',\n",
              " 'pixel648',\n",
              " 'pixel649',\n",
              " 'pixel650',\n",
              " 'pixel651',\n",
              " 'pixel652',\n",
              " 'pixel653',\n",
              " 'pixel654',\n",
              " 'pixel655',\n",
              " 'pixel656',\n",
              " 'pixel657',\n",
              " 'pixel658',\n",
              " 'pixel659',\n",
              " 'pixel660',\n",
              " 'pixel661',\n",
              " 'pixel662',\n",
              " 'pixel663',\n",
              " 'pixel664',\n",
              " 'pixel665',\n",
              " 'pixel666',\n",
              " 'pixel667',\n",
              " 'pixel668',\n",
              " 'pixel669',\n",
              " 'pixel670',\n",
              " 'pixel671',\n",
              " 'pixel672',\n",
              " 'pixel673',\n",
              " 'pixel674',\n",
              " 'pixel675',\n",
              " 'pixel676',\n",
              " 'pixel677',\n",
              " 'pixel678',\n",
              " 'pixel679',\n",
              " 'pixel680',\n",
              " 'pixel681',\n",
              " 'pixel682',\n",
              " 'pixel683',\n",
              " 'pixel684',\n",
              " 'pixel685',\n",
              " 'pixel686',\n",
              " 'pixel687',\n",
              " 'pixel688',\n",
              " 'pixel689',\n",
              " 'pixel690',\n",
              " 'pixel691',\n",
              " 'pixel692',\n",
              " 'pixel693',\n",
              " 'pixel694',\n",
              " 'pixel695',\n",
              " 'pixel696',\n",
              " 'pixel697',\n",
              " 'pixel698',\n",
              " 'pixel699',\n",
              " 'pixel700',\n",
              " 'pixel701',\n",
              " 'pixel702',\n",
              " 'pixel703',\n",
              " 'pixel704',\n",
              " 'pixel705',\n",
              " 'pixel706',\n",
              " 'pixel707',\n",
              " 'pixel708',\n",
              " 'pixel709',\n",
              " 'pixel710',\n",
              " 'pixel711',\n",
              " 'pixel712',\n",
              " 'pixel713',\n",
              " 'pixel714',\n",
              " 'pixel715',\n",
              " 'pixel716',\n",
              " 'pixel717',\n",
              " 'pixel718',\n",
              " 'pixel719',\n",
              " 'pixel720',\n",
              " 'pixel721',\n",
              " 'pixel722',\n",
              " 'pixel723',\n",
              " 'pixel724',\n",
              " 'pixel725',\n",
              " 'pixel726',\n",
              " 'pixel727',\n",
              " 'pixel728',\n",
              " 'pixel729',\n",
              " 'pixel730',\n",
              " 'pixel731',\n",
              " 'pixel732',\n",
              " 'pixel733',\n",
              " 'pixel734',\n",
              " 'pixel735',\n",
              " 'pixel736',\n",
              " 'pixel737',\n",
              " 'pixel738',\n",
              " 'pixel739',\n",
              " 'pixel740',\n",
              " 'pixel741',\n",
              " 'pixel742',\n",
              " 'pixel743',\n",
              " 'pixel744',\n",
              " 'pixel745',\n",
              " 'pixel746',\n",
              " 'pixel747',\n",
              " 'pixel748',\n",
              " 'pixel749',\n",
              " 'pixel750',\n",
              " 'pixel751',\n",
              " 'pixel752',\n",
              " 'pixel753',\n",
              " 'pixel754',\n",
              " 'pixel755',\n",
              " 'pixel756',\n",
              " 'pixel757',\n",
              " 'pixel758',\n",
              " 'pixel759',\n",
              " 'pixel760',\n",
              " 'pixel761',\n",
              " 'pixel762',\n",
              " 'pixel763',\n",
              " 'pixel764',\n",
              " 'pixel765',\n",
              " 'pixel766',\n",
              " 'pixel767',\n",
              " 'pixel768',\n",
              " 'pixel769',\n",
              " 'pixel770',\n",
              " 'pixel771',\n",
              " 'pixel772',\n",
              " 'pixel773',\n",
              " 'pixel774',\n",
              " 'pixel775',\n",
              " 'pixel776',\n",
              " 'pixel777',\n",
              " 'pixel778',\n",
              " 'pixel779',\n",
              " 'pixel780',\n",
              " 'pixel781',\n",
              " 'pixel782',\n",
              " 'pixel783']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_InVtWCtQxuJ"
      },
      "source": [
        "Break the data into Images and Labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train.iloc[:,1:]\n",
        "y = train['label']"
      ],
      "metadata": {
        "id": "Ne43VjcGhxLH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "m9jOh-kWoLnj",
        "outputId": "cb38a6c5-e584-433e-d7a8-60189ff81b72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0           0       0       0       0       0       0       0       0       0   \n",
              "1           0       0       0       0       0       0       0       0       0   \n",
              "2           0       0       0       0       0       0       0       0       0   \n",
              "3           0       0       0       0       0       0       0       0       0   \n",
              "4           0       0       0       0       0       0       0       0       0   \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "41995       0       0       0       0       0       0       0       0       0   \n",
              "41996       0       0       0       0       0       0       0       0       0   \n",
              "41997       0       0       0       0       0       0       0       0       0   \n",
              "41998       0       0       0       0       0       0       0       0       0   \n",
              "41999       0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
              "0           0  ...         0         0         0         0         0   \n",
              "1           0  ...         0         0         0         0         0   \n",
              "2           0  ...         0         0         0         0         0   \n",
              "3           0  ...         0         0         0         0         0   \n",
              "4           0  ...         0         0         0         0         0   \n",
              "...       ...  ...       ...       ...       ...       ...       ...   \n",
              "41995       0  ...         0         0         0         0         0   \n",
              "41996       0  ...         0         0         0         0         0   \n",
              "41997       0  ...         0         0         0         0         0   \n",
              "41998       0  ...         0         0         0         0         0   \n",
              "41999       0  ...         0         0         0         0         0   \n",
              "\n",
              "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
              "0             0         0         0         0         0  \n",
              "1             0         0         0         0         0  \n",
              "2             0         0         0         0         0  \n",
              "3             0         0         0         0         0  \n",
              "4             0         0         0         0         0  \n",
              "...         ...       ...       ...       ...       ...  \n",
              "41995         0         0         0         0         0  \n",
              "41996         0         0         0         0         0  \n",
              "41997         0         0         0         0         0  \n",
              "41998         0         0         0         0         0  \n",
              "41999         0         0         0         0         0  \n",
              "\n",
              "[42000 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2179500-5350-4584-8530-1651879d82d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42000 rows × 784 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2179500-5350-4584-8530-1651879d82d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2179500-5350-4584-8530-1651879d82d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2179500-5350-4584-8530-1651879d82d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c1673ce-d559-42a9-9fad-745b693909fc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c1673ce-d559-42a9-9fad-745b693909fc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c1673ce-d559-42a9-9fad-745b693909fc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qeHWjeGRvHu"
      },
      "source": [
        "Train Test Split of Images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 123 )"
      ],
      "metadata": {
        "id": "t07H4gyLoTey"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per = Perceptron(verbose = 2)\n",
        "per.fit(x_train,y_train)\n",
        "per_preds_train = per.predict(x_train)\n",
        "per_preds_test = per.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5iACsQBjPC-",
        "outputId": "7e3af812-2ec3-426b-80e9-df3daa2ffa17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 33680.40, NNZs: 599, Bias: -92.000000, T: 33600, Avg. loss: 46817.951667\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42819.49, NNZs: 607, Bias: -161.000000, T: 67200, Avg. loss: 41184.869077\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 48607.75, NNZs: 614, Bias: -222.000000, T: 100800, Avg. loss: 36891.702232\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 54442.08, NNZs: 625, Bias: -281.000000, T: 134400, Avg. loss: 35999.461042\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 58608.01, NNZs: 631, Bias: -333.000000, T: 168000, Avg. loss: 37009.589702\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 61946.25, NNZs: 635, Bias: -383.000000, T: 201600, Avg. loss: 34799.316220\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 65207.63, NNZs: 637, Bias: -432.000000, T: 235200, Avg. loss: 36238.479613\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 68995.89, NNZs: 639, Bias: -476.000000, T: 268800, Avg. loss: 34062.205625\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 72716.99, NNZs: 639, Bias: -521.000000, T: 302400, Avg. loss: 32438.202173\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 75447.16, NNZs: 639, Bias: -563.000000, T: 336000, Avg. loss: 32292.536667\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 77822.41, NNZs: 639, Bias: -609.000000, T: 369600, Avg. loss: 34301.058214\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 81007.01, NNZs: 639, Bias: -661.000000, T: 403200, Avg. loss: 35224.191131\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 82890.94, NNZs: 639, Bias: -705.000000, T: 436800, Avg. loss: 33747.325863\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 84973.26, NNZs: 639, Bias: -748.000000, T: 470400, Avg. loss: 31301.132321\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 87757.36, NNZs: 639, Bias: -792.000000, T: 504000, Avg. loss: 32312.222708\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 89517.12, NNZs: 639, Bias: -834.000000, T: 537600, Avg. loss: 31293.953988\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 92120.81, NNZs: 639, Bias: -879.000000, T: 571200, Avg. loss: 31823.835923\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 93354.02, NNZs: 639, Bias: -923.000000, T: 604800, Avg. loss: 32199.063512\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 95389.80, NNZs: 639, Bias: -962.000000, T: 638400, Avg. loss: 31501.624494\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 97060.82, NNZs: 639, Bias: -1013.000000, T: 672000, Avg. loss: 32789.124375\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 99342.75, NNZs: 639, Bias: -1061.000000, T: 705600, Avg. loss: 33667.573423\n",
            "Total training time: 1.10 seconds.\n",
            "Convergence after 21 epochs took 1.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 26398.31, NNZs: 560, Bias: -34.000000, T: 33600, Avg. loss: 25361.617619\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 31613.19, NNZs: 570, Bias: -58.000000, T: 67200, Avg. loss: 22125.669107\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 36264.55, NNZs: 571, Bias: -73.000000, T: 100800, Avg. loss: 21133.352976\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 38909.07, NNZs: 576, Bias: -84.000000, T: 134400, Avg. loss: 20576.478393\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 41697.16, NNZs: 578, Bias: -98.000000, T: 168000, Avg. loss: 19900.769613\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 44398.44, NNZs: 581, Bias: -105.000000, T: 201600, Avg. loss: 20754.749137\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 46842.38, NNZs: 583, Bias: -118.000000, T: 235200, Avg. loss: 19517.157589\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 49225.67, NNZs: 594, Bias: -128.000000, T: 268800, Avg. loss: 19386.208720\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 51823.38, NNZs: 599, Bias: -137.000000, T: 302400, Avg. loss: 18184.694405\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 54147.67, NNZs: 600, Bias: -143.000000, T: 336000, Avg. loss: 17881.692321\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 56021.15, NNZs: 604, Bias: -148.000000, T: 369600, Avg. loss: 18653.010893\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 58496.56, NNZs: 609, Bias: -153.000000, T: 403200, Avg. loss: 18471.054405\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 59823.30, NNZs: 609, Bias: -163.000000, T: 436800, Avg. loss: 18384.287500\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 61703.24, NNZs: 609, Bias: -173.000000, T: 470400, Avg. loss: 17105.853750\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 63489.23, NNZs: 612, Bias: -181.000000, T: 504000, Avg. loss: 18975.412976\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 65371.44, NNZs: 612, Bias: -186.000000, T: 537600, Avg. loss: 17669.717976\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 66946.91, NNZs: 614, Bias: -194.000000, T: 571200, Avg. loss: 17663.917560\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 68839.28, NNZs: 617, Bias: -203.000000, T: 604800, Avg. loss: 18486.346875\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 70613.40, NNZs: 617, Bias: -213.000000, T: 638400, Avg. loss: 18157.243452\n",
            "Total training time: 0.99 seconds.\n",
            "Convergence after 19 epochs took 0.99 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37314.14, NNZs: 631, Bias: -88.000000, T: 33600, Avg. loss: 96577.045923\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 44128.72, NNZs: 636, Bias: -162.000000, T: 67200, Avg. loss: 90873.737083\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 50583.74, NNZs: 642, Bias: -231.000000, T: 100800, Avg. loss: 89628.583750\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 54168.58, NNZs: 643, Bias: -302.000000, T: 134400, Avg. loss: 90226.199018\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 56510.36, NNZs: 643, Bias: -357.000000, T: 168000, Avg. loss: 86301.561548\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 62285.13, NNZs: 644, Bias: -425.000000, T: 201600, Avg. loss: 88602.669762\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 63389.29, NNZs: 644, Bias: -489.000000, T: 235200, Avg. loss: 87977.359286\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 66491.61, NNZs: 644, Bias: -556.000000, T: 268800, Avg. loss: 85774.638423\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 69262.00, NNZs: 644, Bias: -609.000000, T: 302400, Avg. loss: 87651.091399\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 71601.70, NNZs: 644, Bias: -668.000000, T: 336000, Avg. loss: 85173.424613\n",
            "Total training time: 0.52 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 74203.18, NNZs: 645, Bias: -723.000000, T: 369600, Avg. loss: 86586.834167\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 75418.48, NNZs: 646, Bias: -781.000000, T: 403200, Avg. loss: 84865.833690\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 76865.98, NNZs: 646, Bias: -832.000000, T: 436800, Avg. loss: 90802.518750\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 78601.91, NNZs: 646, Bias: -887.000000, T: 470400, Avg. loss: 89849.372232\n",
            "Total training time: 0.73 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 81254.93, NNZs: 650, Bias: -951.000000, T: 504000, Avg. loss: 84589.548869\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 82067.45, NNZs: 649, Bias: -1013.000000, T: 537600, Avg. loss: 88280.016190\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 84184.62, NNZs: 650, Bias: -1075.000000, T: 571200, Avg. loss: 85966.068036\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 85028.95, NNZs: 650, Bias: -1128.000000, T: 604800, Avg. loss: 88452.863333\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 86305.68, NNZs: 650, Bias: -1185.000000, T: 638400, Avg. loss: 83922.722500\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 87377.31, NNZs: 650, Bias: -1242.000000, T: 672000, Avg. loss: 87437.992798\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 88789.49, NNZs: 654, Bias: -1291.000000, T: 705600, Avg. loss: 86266.221339\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 89872.88, NNZs: 654, Bias: -1346.000000, T: 739200, Avg. loss: 84863.819226\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 90691.38, NNZs: 655, Bias: -1392.000000, T: 772800, Avg. loss: 84192.695446\n",
            "Total training time: 1.20 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 92075.79, NNZs: 654, Bias: -1456.000000, T: 806400, Avg. loss: 89009.586488\n",
            "Total training time: 1.25 seconds.\n",
            "Convergence after 24 epochs took 1.25 seconds\n",
            "-- Epoch 1\n",
            "Norm: 35843.44, NNZs: 592, Bias: -183.000000, T: 33600, Avg. loss: 128480.352351\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 42126.43, NNZs: 601, Bias: -326.000000, T: 67200, Avg. loss: 129710.005357\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 47657.56, NNZs: 606, Bias: -466.000000, T: 100800, Avg. loss: 121485.599970\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 53381.81, NNZs: 608, Bias: -589.000000, T: 134400, Avg. loss: 124830.879792\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 57446.25, NNZs: 610, Bias: -717.000000, T: 168000, Avg. loss: 123217.713780\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 59583.12, NNZs: 610, Bias: -832.000000, T: 201600, Avg. loss: 124839.120506\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 61814.28, NNZs: 611, Bias: -969.000000, T: 235200, Avg. loss: 124496.203720\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 64293.43, NNZs: 611, Bias: -1105.000000, T: 268800, Avg. loss: 121835.435774\n",
            "Total training time: 0.42 seconds.\n",
            "Convergence after 8 epochs took 0.42 seconds\n",
            "-- Epoch 1\n",
            "Norm: 37641.17, NNZs: 632, Bias: -63.000000, T: 33600, Avg. loss: 80582.521369\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45561.48, NNZs: 643, Bias: -110.000000, T: 67200, Avg. loss: 67452.582917\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51531.62, NNZs: 657, Bias: -140.000000, T: 100800, Avg. loss: 65367.227470\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56796.33, NNZs: 660, Bias: -176.000000, T: 134400, Avg. loss: 61077.277024\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 61779.10, NNZs: 661, Bias: -220.000000, T: 168000, Avg. loss: 62217.314673\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 66212.86, NNZs: 665, Bias: -257.000000, T: 201600, Avg. loss: 62441.151220\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 70281.72, NNZs: 665, Bias: -301.000000, T: 235200, Avg. loss: 60719.254881\n",
            "Total training time: 0.37 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 73357.68, NNZs: 666, Bias: -339.000000, T: 268800, Avg. loss: 59862.561071\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 75843.56, NNZs: 668, Bias: -382.000000, T: 302400, Avg. loss: 59100.903899\n",
            "Total training time: 0.47 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 78748.74, NNZs: 668, Bias: -416.000000, T: 336000, Avg. loss: 60639.541577\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 80260.03, NNZs: 668, Bias: -447.000000, T: 369600, Avg. loss: 59565.494315\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 82765.54, NNZs: 668, Bias: -475.000000, T: 403200, Avg. loss: 58853.275179\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 84879.71, NNZs: 668, Bias: -510.000000, T: 436800, Avg. loss: 60189.332113\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 86717.81, NNZs: 668, Bias: -555.000000, T: 470400, Avg. loss: 59021.259107\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 88321.25, NNZs: 668, Bias: -590.000000, T: 504000, Avg. loss: 60514.877024\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 90614.48, NNZs: 670, Bias: -625.000000, T: 537600, Avg. loss: 58115.752589\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 92847.08, NNZs: 670, Bias: -663.000000, T: 571200, Avg. loss: 60877.726012\n",
            "Total training time: 0.89 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 94550.09, NNZs: 671, Bias: -694.000000, T: 604800, Avg. loss: 57533.169613\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 95684.10, NNZs: 671, Bias: -728.000000, T: 638400, Avg. loss: 59977.879911\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 96928.58, NNZs: 671, Bias: -761.000000, T: 672000, Avg. loss: 60506.035565\n",
            "Total training time: 1.04 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 98871.28, NNZs: 671, Bias: -799.000000, T: 705600, Avg. loss: 57912.343601\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 99679.73, NNZs: 671, Bias: -832.000000, T: 739200, Avg. loss: 60952.210387\n",
            "Total training time: 1.15 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 100897.16, NNZs: 672, Bias: -876.000000, T: 772800, Avg. loss: 61936.474762\n",
            "Total training time: 1.20 seconds.\n",
            "Convergence after 23 epochs took 1.20 seconds\n",
            "-- Epoch 1\n",
            "Norm: 42423.66, NNZs: 606, Bias: 27.000000, T: 33600, Avg. loss: 130188.282381\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 51881.43, NNZs: 619, Bias: 55.000000, T: 67200, Avg. loss: 119434.731429\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 57337.03, NNZs: 628, Bias: 90.000000, T: 100800, Avg. loss: 113136.772054\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 62414.69, NNZs: 639, Bias: 123.000000, T: 134400, Avg. loss: 113143.845446\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 68023.43, NNZs: 640, Bias: 158.000000, T: 168000, Avg. loss: 114112.914911\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 71561.46, NNZs: 641, Bias: 189.000000, T: 201600, Avg. loss: 111591.199643\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 75066.78, NNZs: 641, Bias: 217.000000, T: 235200, Avg. loss: 110774.543542\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 77962.25, NNZs: 641, Bias: 250.000000, T: 268800, Avg. loss: 109275.440357\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 81947.15, NNZs: 641, Bias: 277.000000, T: 302400, Avg. loss: 108801.517976\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 84724.58, NNZs: 641, Bias: 314.000000, T: 336000, Avg. loss: 110661.619821\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 87225.59, NNZs: 641, Bias: 354.000000, T: 369600, Avg. loss: 108303.698780\n",
            "Total training time: 0.59 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 89073.29, NNZs: 641, Bias: 389.000000, T: 403200, Avg. loss: 106832.891131\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 91419.32, NNZs: 641, Bias: 416.000000, T: 436800, Avg. loss: 110941.566429\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 93994.71, NNZs: 641, Bias: 456.000000, T: 470400, Avg. loss: 113329.592262\n",
            "Total training time: 0.75 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 95985.17, NNZs: 641, Bias: 495.000000, T: 504000, Avg. loss: 109959.708244\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 97348.95, NNZs: 642, Bias: 524.000000, T: 537600, Avg. loss: 110222.617411\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 99584.26, NNZs: 641, Bias: 554.000000, T: 571200, Avg. loss: 106344.273869\n",
            "Total training time: 0.91 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 101776.71, NNZs: 642, Bias: 584.000000, T: 604800, Avg. loss: 106643.082440\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 103208.53, NNZs: 642, Bias: 613.000000, T: 638400, Avg. loss: 108343.745804\n",
            "Total training time: 1.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 105713.85, NNZs: 642, Bias: 639.000000, T: 672000, Avg. loss: 110997.620417\n",
            "Total training time: 1.06 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 107202.41, NNZs: 642, Bias: 672.000000, T: 705600, Avg. loss: 106189.241994\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 109639.44, NNZs: 642, Bias: 698.000000, T: 739200, Avg. loss: 105599.439702\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 110812.71, NNZs: 642, Bias: 732.000000, T: 772800, Avg. loss: 107623.893661\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 112581.36, NNZs: 642, Bias: 765.000000, T: 806400, Avg. loss: 107060.136429\n",
            "Total training time: 1.28 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 114309.41, NNZs: 642, Bias: 798.000000, T: 840000, Avg. loss: 107680.858452\n",
            "Total training time: 1.33 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 116237.10, NNZs: 642, Bias: 826.000000, T: 873600, Avg. loss: 108127.371905\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 117243.89, NNZs: 642, Bias: 861.000000, T: 907200, Avg. loss: 107025.967768\n",
            "Total training time: 1.44 seconds.\n",
            "Convergence after 27 epochs took 1.44 seconds\n",
            "-- Epoch 1\n",
            "Norm: 32486.64, NNZs: 577, Bias: -105.000000, T: 33600, Avg. loss: 61757.694315\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 40534.54, NNZs: 584, Bias: -200.000000, T: 67200, Avg. loss: 56512.068690\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 44742.80, NNZs: 588, Bias: -287.000000, T: 100800, Avg. loss: 56831.399375\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 48306.09, NNZs: 589, Bias: -365.000000, T: 134400, Avg. loss: 57471.921488\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 51911.01, NNZs: 594, Bias: -437.000000, T: 168000, Avg. loss: 53519.729940\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 54449.70, NNZs: 594, Bias: -512.000000, T: 201600, Avg. loss: 56500.563304\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 56208.93, NNZs: 594, Bias: -591.000000, T: 235200, Avg. loss: 55719.122857\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58646.84, NNZs: 595, Bias: -667.000000, T: 268800, Avg. loss: 52715.723690\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 61872.61, NNZs: 600, Bias: -742.000000, T: 302400, Avg. loss: 54451.783214\n",
            "Total training time: 0.46 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 64386.40, NNZs: 600, Bias: -819.000000, T: 336000, Avg. loss: 54228.641726\n",
            "Total training time: 0.51 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 66294.61, NNZs: 600, Bias: -896.000000, T: 369600, Avg. loss: 52856.449167\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 67818.78, NNZs: 600, Bias: -967.000000, T: 403200, Avg. loss: 52751.668839\n",
            "Total training time: 0.62 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 69092.91, NNZs: 600, Bias: -1041.000000, T: 436800, Avg. loss: 51681.258036\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 71238.41, NNZs: 600, Bias: -1116.000000, T: 470400, Avg. loss: 51981.144881\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 72874.40, NNZs: 600, Bias: -1189.000000, T: 504000, Avg. loss: 50819.850149\n",
            "Total training time: 0.77 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 74450.89, NNZs: 600, Bias: -1263.000000, T: 537600, Avg. loss: 53628.265268\n",
            "Total training time: 0.82 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 76487.74, NNZs: 601, Bias: -1337.000000, T: 571200, Avg. loss: 52789.735744\n",
            "Total training time: 0.88 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 77849.43, NNZs: 603, Bias: -1410.000000, T: 604800, Avg. loss: 52466.989940\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 79984.11, NNZs: 603, Bias: -1479.000000, T: 638400, Avg. loss: 53286.368958\n",
            "Total training time: 1.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 80396.01, NNZs: 603, Bias: -1550.000000, T: 672000, Avg. loss: 51882.202679\n",
            "Total training time: 1.06 seconds.\n",
            "Convergence after 20 epochs took 1.06 seconds\n",
            "-- Epoch 1\n",
            "Norm: 32589.46, NNZs: 583, Bias: -17.000000, T: 33600, Avg. loss: 62025.632560\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 37511.93, NNZs: 609, Bias: -29.000000, T: 67200, Avg. loss: 58115.097619\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 42262.38, NNZs: 613, Bias: -45.000000, T: 100800, Avg. loss: 57995.263839\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 46423.57, NNZs: 620, Bias: -57.000000, T: 134400, Avg. loss: 58623.844851\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 50025.21, NNZs: 622, Bias: -63.000000, T: 168000, Avg. loss: 53911.482560\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 53211.32, NNZs: 620, Bias: -77.000000, T: 201600, Avg. loss: 51937.196726\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 56199.23, NNZs: 624, Bias: -82.000000, T: 235200, Avg. loss: 54733.387946\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 58992.86, NNZs: 625, Bias: -103.000000, T: 268800, Avg. loss: 52753.839226\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 61704.82, NNZs: 627, Bias: -111.000000, T: 302400, Avg. loss: 55863.080506\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 63877.60, NNZs: 632, Bias: -131.000000, T: 336000, Avg. loss: 54647.428988\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 66299.11, NNZs: 634, Bias: -148.000000, T: 369600, Avg. loss: 54471.431637\n",
            "Total training time: 0.66 seconds.\n",
            "Convergence after 11 epochs took 0.66 seconds\n",
            "-- Epoch 1\n",
            "Norm: 41758.03, NNZs: 625, Bias: -462.000000, T: 33600, Avg. loss: 233095.000774\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 50310.57, NNZs: 631, Bias: -882.000000, T: 67200, Avg. loss: 228599.348750\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 56368.42, NNZs: 641, Bias: -1299.000000, T: 100800, Avg. loss: 227184.443333\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 60403.37, NNZs: 645, Bias: -1688.000000, T: 134400, Avg. loss: 229155.082083\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63372.61, NNZs: 649, Bias: -2067.000000, T: 168000, Avg. loss: 221451.627738\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 67369.73, NNZs: 650, Bias: -2466.000000, T: 201600, Avg. loss: 225979.830327\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 70718.75, NNZs: 650, Bias: -2852.000000, T: 235200, Avg. loss: 225628.723304\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 71812.83, NNZs: 651, Bias: -3252.000000, T: 268800, Avg. loss: 226431.173661\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 73696.43, NNZs: 653, Bias: -3628.000000, T: 302400, Avg. loss: 223407.070655\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 76871.33, NNZs: 653, Bias: -3996.000000, T: 336000, Avg. loss: 223646.291935\n",
            "Total training time: 0.63 seconds.\n",
            "Convergence after 10 epochs took 0.63 seconds\n",
            "-- Epoch 1\n",
            "Norm: 36290.84, NNZs: 625, Bias: -187.000000, T: 33600, Avg. loss: 159959.056161\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 44920.30, NNZs: 641, Bias: -378.000000, T: 67200, Avg. loss: 154155.718125\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 51342.61, NNZs: 651, Bias: -555.000000, T: 100800, Avg. loss: 150645.519911\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 56524.62, NNZs: 652, Bias: -729.000000, T: 134400, Avg. loss: 146213.910208\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 59759.74, NNZs: 652, Bias: -916.000000, T: 168000, Avg. loss: 150237.405774\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 63784.97, NNZs: 652, Bias: -1087.000000, T: 201600, Avg. loss: 147086.445833\n",
            "Total training time: 0.38 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 65860.29, NNZs: 653, Bias: -1268.000000, T: 235200, Avg. loss: 149149.701250\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 67339.26, NNZs: 656, Bias: -1434.000000, T: 268800, Avg. loss: 150210.671667\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 69460.15, NNZs: 656, Bias: -1616.000000, T: 302400, Avg. loss: 147811.250506\n",
            "Total training time: 0.57 seconds.\n",
            "Convergence after 9 epochs took 0.57 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(actual, predicted):\n",
        "  conf_matrix = confusion_matrix(actual,predicted)\n",
        "  clas_rep = classification_report(actual, predicted)\n",
        "  acc_score = accuracy_score(actual, predicted)\n",
        "  print(\"The model accuracy is: \", acc_score)\n",
        "  print(conf_matrix)\n",
        "  print(clas_rep)"
      ],
      "metadata": {
        "id": "-NGHUjmykJbY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_train,per_preds_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kTQlJqYo5zC",
        "outputId": "24ffb3eb-958d-43f8-8601-fd6db6261054"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is:  0.8858333333333334\n",
            "[[3197    0   14    5    2   31   70    1   25    2]\n",
            " [   1 3556   20    9    2    5    4    4  130    4]\n",
            " [  18    9 2961   70    9   24   34   16  193    5]\n",
            " [  19   10   83 3022    1  142   27   16  152   21]\n",
            " [   5    8  123   17 2912   19   38   17  103   34]\n",
            " [  27    3   16   78   19 2525   96    7  194   30]\n",
            " [  13    5   28    3    5   45 3122    0   46    0]\n",
            " [   7    6  134   17    7    7    6 3269   40   48]\n",
            " [  18   20   38   53    8   92   17   10 2978   12]\n",
            " [  19   15  120   63  188  119    8  298  309 2222]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96      3347\n",
            "           1       0.98      0.95      0.97      3735\n",
            "           2       0.84      0.89      0.86      3339\n",
            "           3       0.91      0.87      0.88      3493\n",
            "           4       0.92      0.89      0.91      3276\n",
            "           5       0.84      0.84      0.84      2995\n",
            "           6       0.91      0.96      0.93      3267\n",
            "           7       0.90      0.92      0.91      3541\n",
            "           8       0.71      0.92      0.80      3246\n",
            "           9       0.93      0.66      0.77      3361\n",
            "\n",
            "    accuracy                           0.89     33600\n",
            "   macro avg       0.89      0.88      0.88     33600\n",
            "weighted avg       0.89      0.89      0.89     33600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi layer Perceptron"
      ],
      "metadata": {
        "id": "QFfIIQMpoLqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes = (16,16,16), verbose = True)"
      ],
      "metadata": {
        "id": "l7lNjeiCpQfs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ge3zD8zLpQgo",
        "outputId": "55f202e9-1ecf-49d7-8a3c-8a4dba48c0fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 4.30935443\n",
            "Iteration 2, loss = 1.56700528\n",
            "Iteration 3, loss = 1.20602526\n",
            "Iteration 4, loss = 0.89781453\n",
            "Iteration 5, loss = 0.73069532\n",
            "Iteration 6, loss = 0.63613692\n",
            "Iteration 7, loss = 0.56300380\n",
            "Iteration 8, loss = 0.50886918\n",
            "Iteration 9, loss = 0.46842120\n",
            "Iteration 10, loss = 0.43210393\n",
            "Iteration 11, loss = 0.40826127\n",
            "Iteration 12, loss = 0.38586368\n",
            "Iteration 13, loss = 0.36693653\n",
            "Iteration 14, loss = 0.35231145\n",
            "Iteration 15, loss = 0.33806958\n",
            "Iteration 16, loss = 0.32432618\n",
            "Iteration 17, loss = 0.31773524\n",
            "Iteration 18, loss = 0.30555399\n",
            "Iteration 19, loss = 0.29313719\n",
            "Iteration 20, loss = 0.28137360\n",
            "Iteration 21, loss = 0.26973006\n",
            "Iteration 22, loss = 0.26557538\n",
            "Iteration 23, loss = 0.26050484\n",
            "Iteration 24, loss = 0.25538465\n",
            "Iteration 25, loss = 0.24806150\n",
            "Iteration 26, loss = 0.24332182\n",
            "Iteration 27, loss = 0.24072519\n",
            "Iteration 28, loss = 0.23801890\n",
            "Iteration 29, loss = 0.23170076\n",
            "Iteration 30, loss = 0.22558934\n",
            "Iteration 31, loss = 0.22384682\n",
            "Iteration 32, loss = 0.21852021\n",
            "Iteration 33, loss = 0.21390730\n",
            "Iteration 34, loss = 0.21086976\n",
            "Iteration 35, loss = 0.20884847\n",
            "Iteration 36, loss = 0.20692993\n",
            "Iteration 37, loss = 0.20480517\n",
            "Iteration 38, loss = 0.20366046\n",
            "Iteration 39, loss = 0.19810138\n",
            "Iteration 40, loss = 0.19393077\n",
            "Iteration 41, loss = 0.19172331\n",
            "Iteration 42, loss = 0.19369616\n",
            "Iteration 43, loss = 0.18669003\n",
            "Iteration 44, loss = 0.18104183\n",
            "Iteration 45, loss = 0.18037482\n",
            "Iteration 46, loss = 0.18310810\n",
            "Iteration 47, loss = 0.17775733\n",
            "Iteration 48, loss = 0.17714781\n",
            "Iteration 49, loss = 0.17727650\n",
            "Iteration 50, loss = 0.16865335\n",
            "Iteration 51, loss = 0.16745311\n",
            "Iteration 52, loss = 0.16543366\n",
            "Iteration 53, loss = 0.16791902\n",
            "Iteration 54, loss = 0.16722607\n",
            "Iteration 55, loss = 0.15941961\n",
            "Iteration 56, loss = 0.15890834\n",
            "Iteration 57, loss = 0.16019527\n",
            "Iteration 58, loss = 0.15558378\n",
            "Iteration 59, loss = 0.15576578\n",
            "Iteration 60, loss = 0.15647469\n",
            "Iteration 61, loss = 0.15233841\n",
            "Iteration 62, loss = 0.15181585\n",
            "Iteration 63, loss = 0.14927576\n",
            "Iteration 64, loss = 0.14683677\n",
            "Iteration 65, loss = 0.14421484\n",
            "Iteration 66, loss = 0.14830878\n",
            "Iteration 67, loss = 0.14291660\n",
            "Iteration 68, loss = 0.14531387\n",
            "Iteration 69, loss = 0.14024116\n",
            "Iteration 70, loss = 0.14335977\n",
            "Iteration 71, loss = 0.13969004\n",
            "Iteration 72, loss = 0.14015873\n",
            "Iteration 73, loss = 0.13734223\n",
            "Iteration 74, loss = 0.13574949\n",
            "Iteration 75, loss = 0.13619376\n",
            "Iteration 76, loss = 0.13433961\n",
            "Iteration 77, loss = 0.13298744\n",
            "Iteration 78, loss = 0.13364629\n",
            "Iteration 79, loss = 0.13621251\n",
            "Iteration 80, loss = 0.13050682\n",
            "Iteration 81, loss = 0.13272969\n",
            "Iteration 82, loss = 0.13054540\n",
            "Iteration 83, loss = 0.13037614\n",
            "Iteration 84, loss = 0.12616295\n",
            "Iteration 85, loss = 0.12778952\n",
            "Iteration 86, loss = 0.12680826\n",
            "Iteration 87, loss = 0.12479884\n",
            "Iteration 88, loss = 0.12471893\n",
            "Iteration 89, loss = 0.12511649\n",
            "Iteration 90, loss = 0.12384417\n",
            "Iteration 91, loss = 0.12012691\n",
            "Iteration 92, loss = 0.11961931\n",
            "Iteration 93, loss = 0.11682397\n",
            "Iteration 94, loss = 0.11689829\n",
            "Iteration 95, loss = 0.12495286\n",
            "Iteration 96, loss = 0.11789056\n",
            "Iteration 97, loss = 0.11485917\n",
            "Iteration 98, loss = 0.11829200\n",
            "Iteration 99, loss = 0.11799882\n",
            "Iteration 100, loss = 0.12198726\n",
            "Iteration 101, loss = 0.11366945\n",
            "Iteration 102, loss = 0.11231649\n",
            "Iteration 103, loss = 0.11154134\n",
            "Iteration 104, loss = 0.11316496\n",
            "Iteration 105, loss = 0.11440711\n",
            "Iteration 106, loss = 0.11344063\n",
            "Iteration 107, loss = 0.11088766\n",
            "Iteration 108, loss = 0.11007544\n",
            "Iteration 109, loss = 0.11428522\n",
            "Iteration 110, loss = 0.11249943\n",
            "Iteration 111, loss = 0.10956670\n",
            "Iteration 112, loss = 0.10927936\n",
            "Iteration 113, loss = 0.10465723\n",
            "Iteration 114, loss = 0.11175291\n",
            "Iteration 115, loss = 0.10927682\n",
            "Iteration 116, loss = 0.10743637\n",
            "Iteration 117, loss = 0.10579190\n",
            "Iteration 118, loss = 0.10684183\n",
            "Iteration 119, loss = 0.10518446\n",
            "Iteration 120, loss = 0.10636264\n",
            "Iteration 121, loss = 0.10067509\n",
            "Iteration 122, loss = 0.10248989\n",
            "Iteration 123, loss = 0.10877747\n",
            "Iteration 124, loss = 0.10816973\n",
            "Iteration 125, loss = 0.10611457\n",
            "Iteration 126, loss = 0.10371139\n",
            "Iteration 127, loss = 0.10516505\n",
            "Iteration 128, loss = 0.10096735\n",
            "Iteration 129, loss = 0.10925299\n",
            "Iteration 130, loss = 0.09976576\n",
            "Iteration 131, loss = 0.09841350\n",
            "Iteration 132, loss = 0.09914861\n",
            "Iteration 133, loss = 0.09733851\n",
            "Iteration 134, loss = 0.10194567\n",
            "Iteration 135, loss = 0.10341587\n",
            "Iteration 136, loss = 0.09746146\n",
            "Iteration 137, loss = 0.09331987\n",
            "Iteration 138, loss = 0.09661192\n",
            "Iteration 139, loss = 0.09572296\n",
            "Iteration 140, loss = 0.09276678\n",
            "Iteration 141, loss = 0.09597586\n",
            "Iteration 142, loss = 0.09971059\n",
            "Iteration 143, loss = 0.10336693\n",
            "Iteration 144, loss = 0.09564446\n",
            "Iteration 145, loss = 0.09766296\n",
            "Iteration 146, loss = 0.09814544\n",
            "Iteration 147, loss = 0.09521653\n",
            "Iteration 148, loss = 0.09549245\n",
            "Iteration 149, loss = 0.09625926\n",
            "Iteration 150, loss = 0.09634207\n",
            "Iteration 151, loss = 0.09449899\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(16, 16, 16), verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_mlp_train = mlp.predict(x_train)\n",
        "preds_mlp_test = mlp.predict(x_test)"
      ],
      "metadata": {
        "id": "TASFynORpZs2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_train, preds_mlp_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPORY4aHrOKl",
        "outputId": "936a5d5f-dc3d-4553-84b4-dd0bfdd6dd1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is:  0.9761607142857143\n",
            "[[3293    0    3    0    4    0   22    0   25    0]\n",
            " [   1 3715    4    0    0    0    0    1   13    1]\n",
            " [  20    1 3235   28    3    6    9    7   29    1]\n",
            " [  13    1   52 3328    2   29    4    3   28   33]\n",
            " [   9    0    0    0 3208    0   16    1    1   41]\n",
            " [  10    0    5   26    6 2904   18    1   11   14]\n",
            " [  25    0    2    0    4    5 3226    0    5    0]\n",
            " [   2    4   11    1    7    1    0 3487    0   28]\n",
            " [   6    2   14   34    6   18    9    1 3143   13]\n",
            " [   5    2    0   20   48    6    1    9   10 3260]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98      3347\n",
            "           1       1.00      0.99      1.00      3735\n",
            "           2       0.97      0.97      0.97      3339\n",
            "           3       0.97      0.95      0.96      3493\n",
            "           4       0.98      0.98      0.98      3276\n",
            "           5       0.98      0.97      0.97      2995\n",
            "           6       0.98      0.99      0.98      3267\n",
            "           7       0.99      0.98      0.99      3541\n",
            "           8       0.96      0.97      0.97      3246\n",
            "           9       0.96      0.97      0.97      3361\n",
            "\n",
            "    accuracy                           0.98     33600\n",
            "   macro avg       0.98      0.98      0.98     33600\n",
            "weighted avg       0.98      0.98      0.98     33600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(y_test, preds_mlp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD0CufSBrQ5L",
        "outputId": "5552aa35-417c-4406-bb21-3cff5404c903"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is:  0.9297619047619048\n",
            "[[760   0   5   2   3   4   8   0   3   0]\n",
            " [  0 925   6   3   2   1   1   1   6   4]\n",
            " [  4   4 774  15   7   4   8   8  13   1]\n",
            " [  6   2  23 772   3  25   1   7   9  10]\n",
            " [  2   5   2   0 742   3   6   3   5  28]\n",
            " [  5   4   6  27   2 720  12   0  16   8]\n",
            " [ 12   1   4   0   7  12 827   0   6   1]\n",
            " [  3   5  12   3   2   3   1 800   3  28]\n",
            " [  4   6   9  20   5  11   6   3 740  13]\n",
            " [  3   0   1   4  33   7   1  22   6 750]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       785\n",
            "           1       0.97      0.97      0.97       949\n",
            "           2       0.92      0.92      0.92       838\n",
            "           3       0.91      0.90      0.91       858\n",
            "           4       0.92      0.93      0.93       796\n",
            "           5       0.91      0.90      0.91       800\n",
            "           6       0.95      0.95      0.95       870\n",
            "           7       0.95      0.93      0.94       860\n",
            "           8       0.92      0.91      0.91       817\n",
            "           9       0.89      0.91      0.90       827\n",
            "\n",
            "    accuracy                           0.93      8400\n",
            "   macro avg       0.93      0.93      0.93      8400\n",
            "weighted avg       0.93      0.93      0.93      8400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8pxQ0uQRQ7f"
      },
      "source": [
        "# Keras Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKj9GMcWRedu"
      },
      "source": [
        "Approach to creating models in Keras\n",
        "\n",
        "1.   Initializing a model\n",
        "     1. Sequential Model (Sequence of Layers)\n",
        "     2. Functional Model (Multi modal, Transfer Learning)\n",
        "2.   Set up the layers\n",
        "     1. Dense Layers, Fully Connected Layers, MLP\n",
        "     2. Dropout (Avoids overfitting)\n",
        "3.   Compilation\n",
        "     1. Optimizer (SGD, ADAM)\n",
        "     2. Loss Function (Regression Loss Functions, Classification Loss Func)\n",
        "     3. Metrics (Accuracy)\n",
        "4.   Model Fit\n",
        "     1. X & Y (Train and Test)\n",
        "     2. Epochs (Number of iterations)\n",
        "     3. Batch_Size = 128, 256 - Batch of images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation = 'relu', input_shape = (784,)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "btSkCyxxrWI_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go_Zl4Odd4Gq",
        "outputId": "ea97b4b6-a645-499d-beb3-0bdf983e59a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134794 (526.54 KB)\n",
            "Trainable params: 134794 (526.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxk2TemCd6Gy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}